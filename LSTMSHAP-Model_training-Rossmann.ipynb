{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa7d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --quiet pytorch-lightning\n",
    "#!pip install --quiet tqdm\n",
    "#!pip install shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c5665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns  #Visualization\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import math\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "import pandas as pd   #preprocessing\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import shap\n",
    "import shap.plots\n",
    "\n",
    "\n",
    "\n",
    "import torch          #modelling\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "\n",
    "import operator      #random\n",
    "import os\n",
    "from IPython.utils import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e343dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid',palette='muted',font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = ['#01BEFE','#FFDD00','#FF7D00','#FF006D','#ADFF02','#8F00FF']\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize']=12,8\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "print(shap.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1d6892",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24c8517",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'Predictions/Rossmann/5th_prediction'\n",
    "\n",
    "try:\n",
    "    os.mkdir(f'{folder_path}')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.mkdir(f'{folder_path}/Stores')\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "\n",
    "N_EPOCHS = 15\n",
    "BATCH_SIZE = 7\n",
    "SEQUENCE_LENGTH = 2\n",
    "N_HIDDEN = 64\n",
    "N_LAYERS = 4\n",
    "PATIENCE = 3\n",
    "LEARNING = 0.001\n",
    "\n",
    "\n",
    "FEATURES_TO_USE = 13\n",
    "SHAP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc13fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a dataframe with the selected features, forcing day, month, year and past sales to be used\n",
    "def features_dataframe(df, corr):\n",
    "    rows = []\n",
    "    '''\n",
    "    'Store', 'DayOfWeek', 'Sales', 'Customers', 'Open', 'Promo',\n",
    "       'SchoolHoliday', 'CompetitionDistance', 'CompetitionOpenSinceYear',\n",
    "       'PromoInterval', 'Promo2', 'CompetitionOpenSinceMonth', 'year', 'month',\n",
    "       'day', 'week', 'StoreType_a', 'StoreType_b', 'StoreType_c',\n",
    "       'StoreType_d', 'Assortment_a', 'Assortment_b', 'Assortment_c',\n",
    "       'StateHoliday_0', 'StateHoliday_a', 'StateHoliday_b', 'StateHoliday_c',\n",
    "        'have_competition'\n",
    "    '''\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        row_data = dict(\n",
    "            Sales = row.Sales,\n",
    "            year = row.year,\n",
    "            month = row.month,\n",
    "            day = row.day,\n",
    "        )\n",
    "        for column in corr:\n",
    "            row_data[column] = row[column]\n",
    "            \n",
    "        rows.append(row_data)\n",
    "    \n",
    "    features_df = pd.DataFrame(rows)\n",
    "    return features_df\n",
    "\n",
    "\n",
    "#spliits the data in test and train\n",
    "def train_test_spliter(ratio,features_df ):\n",
    "    train_size = int(len(features_df)-ratio)\n",
    "    train_df, test_df = features_df[:train_size], features_df[train_size + 1:]\n",
    "\n",
    "    return train_df, test_df, train_size\n",
    "\n",
    "#applies the minmaxscaler to the test and train dataframes\n",
    "def data_scaler(train_df,test_df):\n",
    "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    scaler = scaler.fit(train_df)\n",
    "\n",
    "    train_df = pd.DataFrame(\n",
    "        scaler.transform(train_df),\n",
    "        index = train_df.index,\n",
    "        columns = train_df.columns\n",
    "        )\n",
    "\n",
    "    test_df = pd.DataFrame(\n",
    "        scaler.transform(test_df),\n",
    "        index = test_df.index,\n",
    "        columns = test_df.columns\n",
    "        )\n",
    "  \n",
    "    return train_df, test_df, scaler\n",
    "\n",
    "#create sequences to be used with lstm\n",
    "def create_sequences (input_data:pd.DataFrame, target_column, sequence_length):\n",
    "    sequences = []\n",
    "    data_size = len(input_data)\n",
    "\n",
    "    for i in (range(data_size - sequence_length)):\n",
    "\n",
    "        sequence = input_data[i:i+sequence_length]\n",
    "\n",
    "        label_position = i + sequence_length\n",
    "        label = input_data.iloc[label_position][target_column]\n",
    "\n",
    "        sequences.append((sequence,label))\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "#reverts the minmaxscaler effect\n",
    "def descale(descaler, values):\n",
    "    values_2d=np.array(values)[:,np.newaxis]\n",
    "    \n",
    "    return descaler.inverse_transform(values_2d).flatten()\n",
    "\n",
    "\n",
    "#transform the {sequence,label} dictionary into \"tabular data\" as a dataframe\n",
    "def dict_to_dataframe(dict_list,seq_size):\n",
    "    \n",
    "    column_names = []\n",
    "    data = []\n",
    "    \n",
    "    for d in dict_list:\n",
    "        features = d['sequence'].numpy().flatten()\n",
    "        label = d['label'].numpy()\n",
    "        data.append(np.concatenate((features, label), axis=None))\n",
    "        \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    for j in range(seq_size):\n",
    "        if seq_size-j >= 10:\n",
    "            day_column_names = str(seq_size-j) + 'day_back'+'_' + train_df.columns\n",
    "            column_names.extend(day_column_names)\n",
    "        else:\n",
    "            day_column_names = '0' + str(seq_size-j) + 'day_back'+'_' + train_df.columns\n",
    "            column_names.extend(day_column_names)\n",
    "        \n",
    "    cols = [column_names + ['target']]\n",
    "    df.columns = cols\n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "    \n",
    "#function used to predict the data values in a way the shap.KernelExplainer function can accept it \n",
    "def LSTM_SHAP(array):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        predictions_shap= list(array[:SEQUENCE_LENGTH,0])\n",
    "        label_shap=[]\n",
    "\n",
    "        train_df = pd.DataFrame(array)\n",
    "        #features_df.columns = features\n",
    "\n",
    "        train_sequences = create_sequences(train_df,0,SEQUENCE_LENGTH)\n",
    "\n",
    "        data_module = BTCPriceDataModule(train_sequences, test_sequences, batch_size = BATCH_SIZE)\n",
    "\n",
    "        train_dataset = BTCDataset(train_sequences)\n",
    "\n",
    "        data_module.setup()\n",
    "\n",
    "        for item in train_dataset:\n",
    "            sequence = item['sequence']\n",
    "            label = item['label']\n",
    "            _,output = trained_model(sequence.unsqueeze(dim=0))\n",
    "            predictions_shap.append(output.item())\n",
    "            label_shap.append(label.item())\n",
    "\n",
    "        output = np.array(predictions_shap)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def return_shap_features(shap_values,train_df, number_to_return):\n",
    "    shapdict = {}\n",
    "    for k in train_df.columns:\n",
    "        shapdict[k] = 0\n",
    "        shapdict[f'{k}_count'] = 0\n",
    "\n",
    "\n",
    "    for i in shap_values:\n",
    "        for f in range(len(train_df.columns)):\n",
    "            shapdict[train_df.columns[f]] += abs(i[f])\n",
    "            shapdict[f'{train_df.columns[f]}_count'] += 1\n",
    "\n",
    "    avg_n0_shap = {}\n",
    "\n",
    "    for key in range(0,len(shapdict.keys()),2):\n",
    "        if shapdict[list(shapdict.keys())[key+1]] != 0 and shapdict[list(shapdict.keys())[key]] != 0:\n",
    "            avg_n0_shap[list(shapdict.keys())[key]] = shapdict[list(shapdict.keys())[key]]/shapdict[list(shapdict.keys())[key+1]]\n",
    "\n",
    "    sorted_avg_shap = dict(sorted(avg_n0_shap.items(), key=operator.itemgetter(1), reverse=True))\n",
    "    \n",
    "    top_features = dict(sorted(avg_n0_shap.items(), key=lambda item: item[1], reverse=True)[:number_to_return])\n",
    "    \n",
    "    return sorted_avg_shap, top_features\n",
    "    \n",
    "###########################################################################################################\n",
    "#______________________________________________Model Creation______________________________________________\n",
    "###########################################################################################################\n",
    "\n",
    "class BTCDataset(Dataset):\n",
    "\n",
    "    def __init__(self,sequences):\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        sequence, label = self.sequences[idx]\n",
    "\n",
    "        return dict(\n",
    "            sequence = torch.Tensor(sequence.to_numpy()),\n",
    "            label = torch.tensor(label).float()\n",
    "        )\n",
    "\n",
    "class BTCPriceDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, train_seqeunces,test_sequences, batch_size=8):\n",
    "        super().__init__()\n",
    "        self.train_sequences = train_sequences\n",
    "        self.test_sequences = test_sequences\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self,stage=None):\n",
    "        self.train_dataset = BTCDataset(self.train_sequences)\n",
    "        self.test_dataset = BTCDataset(self.test_sequences)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size = self.batch_size,\n",
    "            shuffle = False,\n",
    "            num_workers = 0\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size = 1,\n",
    "            shuffle = False,\n",
    "            num_workers = 0\n",
    "        )\n",
    "\n",
    "\n",
    "class PricePredictionModel(nn.Module):\n",
    "    def __init__(self, n_features, n_hidden = N_HIDDEN, n_layers = N_LAYERS):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = n_features,\n",
    "            hidden_size = n_hidden,\n",
    "            batch_first = True,\n",
    "            num_layers = n_layers,\n",
    "            dropout = 0.2\n",
    "        )\n",
    "        self.regressor = nn.Linear(n_hidden,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        self.lstm.flatten_parameters()\n",
    "\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        out = hidden[-1]\n",
    "        \n",
    "        return self.regressor(out)\n",
    "    \n",
    "\n",
    "class BTCPricePredictor(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, n_features: int):\n",
    "        super().__init__()\n",
    "        self.model=PricePredictionModel(n_features)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x, labels= None):\n",
    "        output = self.model(x)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels.unsqueeze(dim=1))\n",
    "        return loss, output\n",
    "    \n",
    "    def training_step(self, batch, batch_index):\n",
    "        sequences = batch['sequence']\n",
    "        labels = batch['label']\n",
    "\n",
    "        loss, outputs = self(sequences, labels)\n",
    "        self.log('train_loss', loss, prog_bar = True, logger=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_index):\n",
    "        sequences = batch['sequence']\n",
    "        labels = batch['label']\n",
    "\n",
    "        loss, outputs = self(sequences, labels)\n",
    "        self.log('val_loss', loss, prog_bar = True, logger=False)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_index):\n",
    "        sequences = batch['sequence']\n",
    "        labels = batch['label']\n",
    "\n",
    "        loss, outputs = self(sequences, labels)\n",
    "        self.log('test_loss', loss, prog_bar = True, logger=False)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(self.parameters(), lr = LEARNING)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f987910b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import data\n",
    "\n",
    "df_og = pd.read_csv('Rossmann_treated.csv')\n",
    "#df_og = df_og[df_og['Store']==1]\n",
    "#df_og = df_og.sort_values(by='date').reset_index(drop=True)\n",
    "#products_in_store = np.sort(df_og['product_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb53bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes SHAP calculations for stores inside the rossmann_treated dataset\n",
    "if SHAP == True:\n",
    "    for store in tqdm(df_og['Store'].unique()):\n",
    "        #to omit outputs\n",
    "        with io.capture_output() as captured:\n",
    "            sales_dependencies = {}\n",
    "            dic = {}\n",
    "            df = df_og[df_og['Store'] == store].reset_index(drop = True)\n",
    "            \n",
    "            #return all columns names ('features') except for customers, since it's not an available \n",
    "            #information for future points\n",
    "            features  = ((df.select_dtypes(include='float').dropna(axis=1, how='all')).drop(columns='Customers')).columns.tolist()\n",
    "            #returns dataframe with the features to be analised\n",
    "            features_df = features_dataframe(df,features)\n",
    "            #split into test and train and minmaxscaler\n",
    "            train_df, test_df, train_size =  train_test_spliter(99,features_df)\n",
    "            train_df, test_df, scaler = data_scaler(train_df,test_df)\n",
    "            #make sequences with the data\n",
    "            train_sequences = create_sequences(train_df,'Sales',SEQUENCE_LENGTH)\n",
    "            test_sequences = create_sequences (test_df,'Sales',SEQUENCE_LENGTH)\n",
    "            \n",
    "            #trains the model and store the most recent checkpoints removing previous ones if existing\n",
    "            data_module = BTCPriceDataModule(train_sequences, test_sequences, batch_size = BATCH_SIZE)\n",
    "            data_module.setup()\n",
    "            train_dataset = BTCDataset(train_sequences)\n",
    "            test_dataset = BTCDataset(test_sequences)\n",
    "            model = BTCPricePredictor(n_features = train_df.shape[1])\n",
    "            \n",
    "            try:\n",
    "                os.remove(f\"{folder_path}/CheckpointsSHAP/Rosmannn_shapmodel_store{int(store)}.ckpt\")\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            checkpoint_callback = ModelCheckpoint(\n",
    "                dirpath = f'{folder_path}/CheckpointsSHAP',\n",
    "                filename = f'Rosmannn_shapmodel_store{int(store)}',\n",
    "                save_top_k = 1,\n",
    "                verbose = False ,\n",
    "                monitor = 'val_loss',\n",
    "                mode = 'min'\n",
    "            )\n",
    "            logger = TensorBoardLogger('lightning_logs', name = 'btc-price')\n",
    "            early_stopping_callback = EarlyStopping(monitor= 'val_loss', patience = PATIENCE)\n",
    "            trainer = pl.Trainer(\n",
    "                logger = logger,\n",
    "                callbacks=[early_stopping_callback, checkpoint_callback],\n",
    "                max_epochs = N_EPOCHS,\n",
    "                gpus = 0,\n",
    "            )\n",
    "            trainer.fit(model, data_module)\n",
    "            \n",
    "            #load the best model from checkpoint\n",
    "            trained_model = BTCPricePredictor.load_from_checkpoint(\n",
    "            f'{folder_path}/CheckpointsSHAP/Rosmannn_shapmodel_store{int(store)}.ckpt',\n",
    "            n_features = train_df.shape[1]\n",
    "            )\n",
    "            #uses kmeans to reduce dimentionality of the data while losing the least information possible\n",
    "            kmeans = shap.kmeans(train_df,3)\n",
    "            #explainer\n",
    "            explainer = shap.KernelExplainer(LSTM_SHAP, kmeans)\n",
    "            #explanation\n",
    "            shap_values = explainer.shap_values(train_df[-365:])\n",
    "            #return a FEATURES_TO_USE numpber of features ordered by average shap value\n",
    "            sorted_avg_shap, top_features_dic = return_shap_features(shap_values,train_df,FEATURES_TO_USE)\n",
    "            try:\n",
    "                os.mkdir(f'{folder_path}/Stores/store{int(store)}')\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        shap_per_feature = pd.DataFrame.from_dict(sorted_avg_shap, orient='index', columns=['Importance'])\n",
    "        shap_per_feature.to_csv(f\"{folder_path}/Stores/store{int(store)}/shap_values_{int(store)}.csv\")\n",
    "        \n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050688ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_ds = []\n",
    "must_use= ['year', 'month','day']\n",
    "#retrain model, now with selected features\n",
    "for store in tqdm(df_og['Store'].unique()):\n",
    "    with io.capture_output() as captured:\n",
    "        #imports the important features exported as csv\n",
    "        top_features_df = pd.read_csv(f\"{folder_path}/Stores/store{int(store)}/shap_values_{int(store)}.csv\")\n",
    "\n",
    "        top_features = list(top_features_df.iloc[:,0])\n",
    "\n",
    "        top_features.extend(must_use)\n",
    "        #remove possible duplicates\n",
    "        top_features = list(set(top_features)) \n",
    "\n",
    "        features_df = features_dataframe(df,top_features)\n",
    "\n",
    "        train_df, test_df, train_size =  train_test_spliter(0.85,features_df)\n",
    "        train_df, test_df, scaler = data_scaler(train_df,test_df)\n",
    "\n",
    "        train_sequences = create_sequences(train_df,'Sales',SEQUENCE_LENGTH)\n",
    "        test_sequences = create_sequences (test_df,'Sales',SEQUENCE_LENGTH)\n",
    "        data_module = BTCPriceDataModule(train_sequences, test_sequences, batch_size = BATCH_SIZE)\n",
    "        data_module.setup()\n",
    "\n",
    "        train_dataset = BTCDataset(train_sequences)\n",
    "        test_dataset = BTCDataset(test_sequences)\n",
    "\n",
    "        model = BTCPricePredictor(n_features = train_df.shape[1])\n",
    "\n",
    "        try:\n",
    "            os.remove(f\"{folder_path}/Checkpoints/Rosmannn_prediction_store{int(store)}.ckpt\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath = f'{folder_path}/Checkpoints',\n",
    "            filename = f'Rosmannn_prediction_store{int(store)}',\n",
    "            save_top_k = 1,\n",
    "            verbose = False ,\n",
    "            monitor = 'val_loss',\n",
    "            mode = 'min'\n",
    "        )\n",
    "\n",
    "        logger = TensorBoardLogger('lightning_logs', name = 'btc-price')\n",
    "        early_stopping_callback = EarlyStopping(monitor= 'val_loss', patience = PATIENCE)\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            logger = logger,\n",
    "            callbacks=[early_stopping_callback, checkpoint_callback],\n",
    "            max_epochs = N_EPOCHS,\n",
    "            gpus = 0,\n",
    "        )\n",
    "\n",
    "        trainer.fit(model, data_module)\n",
    "\n",
    "        trained_model = BTCPricePredictor.load_from_checkpoint(\n",
    "        f'{folder_path}/Checkpoints/Rosmannn_prediction_store{int(store)}.ckpt',\n",
    "        n_features = train_df.shape[1]\n",
    "        )\n",
    "\n",
    "        test_dataset = BTCDataset(test_sequences)\n",
    "\n",
    "        predictions = []\n",
    "        labels = []\n",
    "\n",
    "        #data = dict_to_dataframe(list(iter(train_dataset)),SEQUENCE_LENGTH)\n",
    "        #sales_store_corr = data.corr()\n",
    "\n",
    "        #display(sales_store_corr)\n",
    "\n",
    "        for item in test_dataset:\n",
    "            sequence = item['sequence']\n",
    "            label = item['label']\n",
    "            _,output = trained_model(sequence.unsqueeze(dim=0))\n",
    "            predictions.append(output.item())\n",
    "            labels.append(label.item())\n",
    "\n",
    "        descaler = MinMaxScaler()\n",
    "        descaler.min_, descaler.scale_ = scaler.min_[0], scaler.scale_[0]\n",
    "\n",
    "        predictions_descaled = descale(descaler,predictions)\n",
    "        labels_descaled = descale(descaler,labels)\n",
    "\n",
    "        test_data = df[train_size+1:]\n",
    "        test_sequences_data = test_data.iloc[SEQUENCE_LENGTH:]\n",
    "\n",
    "        dates = matplotlib.dates.date2num(test_sequences_data.Date.tolist())\n",
    "        full_dates = matplotlib.dates.date2num(df.Date.tolist())\n",
    "\n",
    "        predictions_descaled = np.where(predictions_descaled<0, 0, predictions_descaled)\n",
    "\n",
    "        try:\n",
    "            os.mkdir(f'{folder_path}/Stores/store{int(store)}')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        dic = {}\n",
    "\n",
    "        dic[f'store{store}_truth'] = df['Sales']\n",
    "        dic[f'store{store}_truth_dates'] = full_dates\n",
    "\n",
    "        truth_df = pd.DataFrame.from_dict(dic)\n",
    "        truth_df.to_csv(f\"{folder_path}/Stores/store{int(store)}/truth_store{int(store)}.csv\",index = False)\n",
    "\n",
    "        dic= {}\n",
    "\n",
    "        dic[f'store{store}_pred'] = predictions_descaled\n",
    "        dic[f'store{store}_pred_dates'] = dates\n",
    "\n",
    "        prediction_df = pd.DataFrame.from_dict(dic)\n",
    "        prediction_df.to_csv(f\"{folder_path}/Stores/store{int(store)}/prediction_store{int(store)}.csv\",index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4d2d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Explanation = shap.Explanation(values=shap_values, data=train_df[-365:])\n",
    "shap.plots.beeswarm(Explanation,max_display=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd12fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Explanation = shap.Explanation(values=shap_values, data=train_df[-365:])\n",
    "Explanation.feature_names = list(Explanation.data.columns)\n",
    "shap.plots.heatmap(Explanation,max_display=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560db28e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
